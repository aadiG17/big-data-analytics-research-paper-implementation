# ğŸ’¡ Integrated Framework for Spark-Based Big Data Analytics in Sports Health Monitoring

This project implements the research paper:
**"Integrated Framework for Spark-Based Big Data Analytics in Sports Health Monitoring"**

ğŸ“š **Research Paper**: [Springer Link](https://link.springer.com/article/10.1007/s00500-023-09450-9)  
ğŸ“¦ **Dataset**: [Chronic Kidney Disease (CKD)](https://www.kaggle.com/datasets/mansoordaku/ckdisease)

---

## âš™ï¸ Tech Stack

- ğŸ”¥ Apache Spark (Standalone Cluster on Docker)
- ğŸ PySpark (Data Processing + MLlib)
- ğŸ¤– Machine Learning: XGBoost, Random Forest, Decision Tree, SVM, Logistic Regression, KNN
- ğŸ““ Jupyter Notebook (inside Spark master container)
- ğŸ“ˆ Streamlit Web App for Predictions
- ğŸ—ƒ Hadoop HDFS (optional for distributed storage)

---

## ğŸ›  Setup Instructions

### 1. Install Docker & Docker Compose
```bash
sudo apt update
sudo apt install docker.io docker-compose
sudo usermod -aG docker $USER
newgrp docker
```

### 2. Clone and Setup Project
```bash
git clone <your-repo-url>
cd SparkScript
sudo docker-compose build
```

### 3. Start Spark Cluster
```bash
sudo docker-compose up -d
```

### 4. Access Services
- ğŸ”— Spark Master UI: [http://localhost:9090](http://localhost:9090)
- ğŸ”— Jupyter Notebook: [http://localhost:8888](http://localhost:8888)
- ğŸ”— Spark History Server: [http://localhost:18080](http://localhost:18080)

> âœ… Jupyter will print a token in the console â€” use it to access initially.

---

## ğŸ”¬ Machine Learning Implementation

- Dataset loaded into PySpark, cleaned, and transformed
- Models trained and evaluated using accuracy, F1-score, and classification report
- Models saved as `.pkl` and integrated into Streamlit app

### âœ… ML Models Used:
- Logistic Regression
- Decision Tree
- Random Forest
- K-Nearest Neighbors (KNN)
- Support Vector Machine (SVM)
- **XGBoost** *(default in Streamlit UI)*

---

## ğŸ–¥ Streamlit Prediction Dashboard

### âš™ï¸ Run Locally (outside Docker)
```bash
streamlit run app.py
```

- Choose ML model (via dropdown)
- Enter medical attributes manually
- Click "Predict" to get result

---

## ğŸ“ Hadoop Integration *(Optional)*

You can extend this to a Hadoop-based architecture:
- Setup 1 master + multiple slaves (including across LAN)
- Upload data to HDFS:
```bash
hdfs dfs -put ckd_data.csv /
```
- Load into Spark:
```python
spark.read.csv("hdfs://master:9000/ckd_data.csv", header=True, inferSchema=True)
```

---

## ğŸ“¸ Screenshots to Include

- âœ… Spark Master Dashboard showing active workers
- âœ… Worker Node UI
- âœ… Jupyter Notebook (running training)
- âœ… Streamlit app with prediction results

(ğŸ–¼ Add these in your GitHub repoâ€™s README.md with `![img](path)` syntax)

---

## ğŸ“š Reference

- Spark on Docker: [Medium Article](https://medium.com/@MarinAgli1/setting-up-a-spark-standalone-cluster-on-docker-in-layman-terms-8cbdc9fdd14b)

---

## ğŸ‘¨â€ğŸ’» Author

**Aaditya** â€” Final Year B.Tech | Big Data â€¢ AI â€¢ Cloud

> ğŸ’¬ _This project brings a real-world research paper into practical implementation using Apache Spark, Docker, ML, and Hadoop._

---

## ğŸ” Useful Commands

```bash
# Stop the Spark cluster
sudo docker-compose down

# Restart Spark cluster
sudo docker-compose up -d

# Check logs for container
sudo docker logs <container_name>
```

---

